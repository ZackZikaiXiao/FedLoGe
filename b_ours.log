nohup: ignoring input
/data/zikai/aapfl/pfed_lastest/util/etf_methods.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sparse_etf = torch.tensor(sparse_etf, requires_grad=True)
./temp/fl_acc_cifar10_resnet18_cons_frac0.2_iidFalse_iter500_ep5_lr0.03_N40_10_seed3407_p1_dirichlet0.5_IF0.01_LossCE.txt
Files already downloaded and verified
[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]
12406
Files already downloaded and verified
Into non-iid sampling
clients_sizes:[215, 141, 207, 171, 243, 132, 178, 751, 618, 173, 602, 559, 821, 446, 221, 157, 127, 436, 273, 246, 171, 124, 90, 190, 252, 91, 656, 129, 194, 482, 1106, 528, 243, 102, 215, 267, 348, 153, 172, 176]
training set distribution:
[[ 72  83   9   1  40   5   5   0   0   0]
 [118   3   0   0   4   6   8   0   0   2]
 [ 32   5 132  20   4  13   0   0   0   1]
 [ 91  34  13   6  15   0   0  12   0   0]
 [ 73 128   1  12  13   5   7   1   1   2]
 [ 10  21  19  73   2   6   0   1   0   0]
 [  0  24 115  11   8   4  11   4   1   0]
 [105 484  11  91  36  11   0   0   6   7]
 [570   1  16   1   4  24   0   2   0   0]
 [  0 113  23   0  26   0  10   1   0   0]
 [397 138   7  22   4  21   0   0  12   1]
 [262 146  60   7  57   9  15   0   0   3]
 [581  84  98  20  17   6  12   1   0   2]
 [ 30   1 259 147   3   0   2   3   1   0]
 [ 15 145  33  11   3   2   0  11   1   0]
 [ 57   0  48  17  11   6   5  11   2   0]
 [  2  54   3   0  36  10  11   1   9   1]
 [323  53  41  10   8   0   0   0   1   0]
 [  2 250   8   2   0   2   2   0   4   3]
 [113  22  34   9  50   1   0  17   0   0]
 [ 93   4   3  29   1  31   5   3   2   0]
 [  0  66   0  40  10   0   1   6   0   1]
 [  0  28   6  36  10   1   3   0   4   2]
 [ 99   7  72   0   1   4   1   0   1   5]
 [126  52   6   0  46   2   8   1  11   0]
 [  3   1  59   6   6   0   4  10   1   1]
 [423 132   4  37  17  31   0  10   0   2]
 [  6   1  15  54  19  13  12   8   0   1]
 [  0 114   7   1   0  13  57   1   0   1]
 [428   0   5  21  13   1   1   0  12   1]
 [190 532 336  27   4  10   0   3   2   2]
 [332 110  13  53   0   4  11   1   4   0]
 [152  27  10   4  22   7   5  11   0   5]
 [  2  11  44  31   3   0   3   6   1   1]
 [ 62  79   7  26   0  40   1   0   0   0]
 [  1   1   2 179  43  30  11   0   0   0]
 [125  11 107  11  30  54   2   8   0   0]
 [ 20  14  87   3  14   0   8   2   5   0]
 [ 48  18  32  29  31   6   0   1   1   6]
 [ 37   0  51  30  34   9  11   3   1   0]]
Total size of training set
12406
local test distribution:
[[ 58  66   7   0  32   4   4   0   0   0]
 [ 95   2   0   0   3   4   6   0   0   1]
 [ 25   4 106  16   3  10   0   0   0   0]
 [ 73  27  10   4  12   0   0   9   0   0]
 [ 58 103   0   9  10   4   5   0   0   1]
 [  8  16  15  58   1   4   0   0   0   0]
 [  0  19  92   8   6   3   8   3   0   0]
 [ 84 390   8  73  29   8   0   0   4   5]
 [459   0  12   0   3  19   0   1   0   0]
 [  0  91  18   0  20   0   8   0   0   0]
 [320 111   5  17   3  16   0   0   9   0]
 [211 117  48   5  45   7  12   0   0   2]
 [468  67  78  16  13   4   9   0   0   1]
 [ 24   0 208 118   2   0   1   2   0   0]
 [ 12 116  26   8   2   1   0   8   0   0]
 [ 45   0  38  13   8   4   4   8   1   0]
 [  1  43   2   0  29   8   8   0   7   0]
 [260  42  33   8   6   0   0   0   0   0]
 [  1 201   6   1   0   1   1   0   3   2]
 [ 91  17  27   7  40   0   0  13   0   0]
 [ 74   3   2  23   0  24   4   2   1   0]
 [  0  53   0  32   8   0   0   4   0   0]
 [  0  22   4  29   8   0   2   0   3   1]
 [ 79   5  58   0   0   3   0   0   0   4]
 [101  41   4   0  37   1   6   0   8   0]
 [  2   0  47   4   4   0   3   8   0   0]
 [340 106   3  29  13  24   0   8   0   1]
 [  4   0  12  43  15  10   9   6   0   0]
 [  0  91   5   0   0  10  45   0   0   0]
 [344   0   4  16  10   0   0   0   9   0]
 [153 428 270  21   3   8   0   2   1   1]
 [267  88  10  42   0   3   8   0   3   0]
 [122  21   8   3  17   5   4   8   0   4]
 [  1   8  35  24   2   0   2   4   0   0]
 [ 49  63   5  20   0  32   0   0   0   0]
 [  0   0   1 144  34  24   8   0   0   0]
 [100   8  86   8  24  43   1   6   0   0]
 [ 16  11  70   2  11   0   6   1   4   0]
 [ 38  14  25  23  24   4   0   0   0   4]
 [ 29   0  41  24  27   7   8   2   0   0]]
Total size of testing set
9829
[4012 2394 1429  848  504  295  172   95   53   27]
40
Step 0, Loss 1.2786084413528442
Step 100, Loss 0.2238391637802124
Step 200, Loss -0.46556174755096436
Step 300, Loss -0.9142656326293945
Step 400, Loss -1.203202247619629
Step 500, Loss -1.387484073638916
Step 600, Loss -1.5035239458084106
Step 700, Loss -1.5765867233276367
Step 800, Loss -1.621006727218628
Step 900, Loss -1.647348403930664
Step 1000, Loss -1.6622207164764404
Step 1100, Loss -1.6705503463745117
Step 1200, Loss -1.6746090650558472
Step 1300, Loss -1.6775033473968506
Step 1400, Loss -1.6787492036819458
Step 1500, Loss -1.679220199584961
Step 1600, Loss -1.679751992225647
Step 1700, Loss -1.6800918579101562
Step 1800, Loss -1.6801832914352417
Step 1900, Loss -1.6799860000610352
Step 2000, Loss -1.6802524328231812
Step 2100, Loss -1.6801005601882935
Step 2200, Loss -1.680379867553711
Step 2300, Loss -1.6796640157699585
Step 2400, Loss -1.6806347370147705
Step 2500, Loss -1.6800808906555176
Step 2600, Loss -1.6805005073547363
Step 2700, Loss -1.680245280265808
Step 2800, Loss -1.6802819967269897
Step 2900, Loss -1.6799700260162354
Step 3000, Loss -1.6806120872497559
Step 3100, Loss -1.6803888082504272
Step 3200, Loss -1.680387020111084
Step 3300, Loss -1.6804478168487549
Step 3400, Loss -1.6804683208465576
Step 3500, Loss -1.6804001331329346
Step 3600, Loss -1.680649757385254
Step 3700, Loss -1.6807612180709839
Step 3800, Loss -1.6804527044296265
Step 3900, Loss -1.6806565523147583
Step 4000, Loss -1.6805329322814941
Step 4100, Loss -1.680406093597412
Step 4200, Loss -1.6805802583694458
Step 4300, Loss -1.680472493171692
Step 4400, Loss -1.6805533170700073
Step 4500, Loss -1.680205225944519
Step 4600, Loss -1.680498480796814
Step 4700, Loss -1.6809632778167725
Step 4800, Loss -1.6806552410125732
Step 4900, Loss -1.6803749799728394
Step 5000, Loss -1.6804993152618408
Step 5100, Loss -1.6805917024612427
Step 5200, Loss -1.6808116436004639
Step 5300, Loss -1.6803611516952515
Step 5400, Loss -1.6807653903961182
Step 5500, Loss -1.680922269821167
Step 5600, Loss -1.68047034740448
Step 5700, Loss -1.6805799007415771
Step 5800, Loss -1.680626630783081
Step 5900, Loss -1.6803933382034302
Step 6000, Loss -1.6805232763290405
Step 6100, Loss -1.6802868843078613
Step 6200, Loss -1.6805118322372437
Step 6300, Loss -1.6806334257125854
Step 6400, Loss -1.6805973052978516
Step 6500, Loss -1.6804367303848267
Step 6600, Loss -1.68057382106781
Step 6700, Loss -1.680816650390625
Step 6800, Loss -1.6803386211395264
Step 6900, Loss -1.6809126138687134
Step 7000, Loss -1.6806609630584717
Step 7100, Loss -1.6807255744934082
Step 7200, Loss -1.680690050125122
Step 7300, Loss -1.6806617975234985
Step 7400, Loss -1.6804965734481812
Step 7500, Loss -1.6804282665252686
Step 7600, Loss -1.6803858280181885
Step 7700, Loss -1.6806825399398804
Step 7800, Loss -1.6809651851654053
Step 7900, Loss -1.6804479360580444
Step 8000, Loss -1.6804178953170776
Step 8100, Loss -1.6808669567108154
Step 8200, Loss -1.6807188987731934
Step 8300, Loss -1.6805310249328613
Step 8400, Loss -1.6804677248001099
Step 8500, Loss -1.680780053138733
Step 8600, Loss -1.6808000802993774
Step 8700, Loss -1.6809009313583374
Step 8800, Loss -1.6809113025665283
Step 8900, Loss -1.6805776357650757
Step 9000, Loss -1.680821418762207
Step 9100, Loss -1.680633783340454
Step 9200, Loss -1.6807537078857422
Step 9300, Loss -1.680574893951416
Step 9400, Loss -1.680887222290039
Step 9500, Loss -1.6806994676589966
Step 9600, Loss -1.6806035041809082
Step 9700, Loss -1.6805094480514526
Step 9800, Loss -1.6805940866470337
Step 9900, Loss -1.6807276010513306
Angle Mean: 96.37919616699219, Angle Variance: 0.0032041105441749096
Norm Mean: 0.10057534277439117, Norm Variance: 5.301112437905431e-09
  0%|          | 0/500 [00:00<?, ?it/s]