nohup: ignoring input
/data/zikai/aapfl/pfed_lastest/util/etf_methods.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sparse_etf = torch.tensor(sparse_etf, requires_grad=True)
./temp/fl_acc_cifar10_resnet18_cons_frac0.2_iidFalse_iter500_ep5_lr0.03_N40_10_seed3407_p1_dirichlet0.5_IF0.02_LossCE.txt
Files already downloaded and verified
[5000, 3237, 2096, 1357, 878, 568, 368, 238, 154, 100]
13996
Files already downloaded and verified
Into non-iid sampling
clients_sizes:[226, 200, 143, 400, 329, 90, 221, 637, 842, 250, 726, 518, 793, 161, 235, 161, 218, 559, 487, 418, 221, 234, 176, 162, 239, 66, 600, 183, 189, 512, 995, 619, 558, 431, 282, 158, 224, 122, 172, 239]
training set distribution:
[[ 72  90   1   0  13   0  23   0  27   0]
 [118   3  12   6  42  10   0   4   0   5]
 [ 32   5   6  43  12   2  22  18   0   3]
 [ 91  37  46  66 110  40   4   2   0   4]
 [ 73 133  59  30   0   0  20   6   0   8]
 [ 10  21  30   1   3  12   0   9   4   0]
 [  0  27  38   0 116  20  15   1   3   1]
 [105 516   3   4   2   0   2   2   0   3]
 [570   1  46  93  87  15   6  19   0   5]
 [  0 120   4  12  59  50   5   0   0   0]
 [397 149 122  11  31   0   0  11   3   2]
 [262 165   1   7   1  51  17   7   6   1]
 [581  93  72   5  21   4   0  10   7   0]
 [ 30   1 104   0   8  10   4   4   0   0]
 [ 15 159  13  19   3  14   5   1   0   6]
 [ 57   0  33  22  17   9   2  15   2   4]
 [  2  59  71  15  42   0  27   0   0   2]
 [323  58  84  37   3   0  26  11   4  13]
 [  2 267 143  18  19  11   0   6  17   4]
 [113  23 172  56   0   3  14  37   0   0]
 [ 93   5  93  13   4   9   1   1   1   1]
 [  0  70  53  43  44   6   7   0  11   0]
 [  0  30  32  90   0   9  11   2   2   0]
 [ 99   8   0   1   2  40   4   1   6   1]
 [126  58  35   0  14   3   2   0   1   0]
 [  3   1  30   4   3  11   5   4   0   5]
 [423 146  13   0   0   1   7   5   3   2]
 [  6   1  70  25  46  34   0   1   0   0]
 [  0 124   8  31   7   0  10   0   5   4]
 [428   0  52   3  17   0   3   6   1   2]
 [190 570   0 173  48   0   4   0   0  10]
 [332 118   6  84   1   6  44   3  21   4]
 [152  29 355   0   6   3  11   0   2   0]
 [  2  11  67 304  13  14   4   1  12   3]
 [ 62  91  85  21  14   4   2   1   1   1]
 [  1   1  35  48  34   0  38   1   0   0]
 [125  12  43   3   2   0  10  16  13   0]
 [ 20  15   0  40   8  29   3   1   1   5]
 [ 48  19  36   0   0  40   7  22   0   0]
 [ 37   1  23  29  26 108   3  10   1   1]]
Total size of training set
13996
local test distribution:
[[ 51  64   0   0   9   0  16   0  19   0]
 [ 84   2   8   4  30   7   0   2   0   3]
 [ 22   3   4  30   8   1  15  12   0   2]
 [ 65  26  32  47  78  28   2   1   0   2]
 [ 52  95  42  21   0   0  14   4   0   5]
 [  7  15  21   0   2   8   0   6   2   0]
 [  0  19  27   0  82  14  10   0   2   0]
 [ 75 368   2   2   1   0   1   1   0   2]
 [407   0  32  66  62  10   4  13   0   3]
 [  0  85   2   8  42  35   3   0   0   0]
 [283 106  87   7  22   0   0   7   2   1]
 [187 117   0   5   0  36  12   5   4   0]
 [415  66  51   3  15   2   0   7   5   0]
 [ 21   0  74   0   5   7   2   2   0   0]
 [ 10 113   9  13   2  10   3   0   0   4]
 [ 40   0  23  15  12   6   1  10   1   2]
 [  1  42  50  10  30   0  19   0   0   1]
 [230  41  60  26   2   0  18   7   2   9]
 [  1 190 102  12  13   7   0   4  12   2]
 [ 80  16 122  40   0   2  10  26   0   0]
 [ 66   3  66   9   2   6   0   0   0   0]
 [  0  50  37  30  31   4   5   0   7   0]
 [  0  21  22  64   0   6   7   1   1   0]
 [ 70   5   0   0   1  28   2   0   4   0]
 [ 90  41  25   0  10   2   1   0   0   0]
 [  2   0  21   2   2   7   3   2   0   3]
 [302 104   9   0   0   0   5   3   2   1]
 [  4   0  50  17  32  24   0   0   0   0]
 [  0  88   5  22   5   0   7   0   3   2]
 [305   0  37   2  12   0   2   4   0   1]
 [135 407   0 123  34   0   2   0   0   7]
 [237  84   4  60   0   4  31   2  15   2]
 [108  20 253   0   4   2   7   0   1   0]
 [  1   7  47 217   9  10   2   0   8   2]
 [ 44  65  60  15  10   2   1   0   0   0]
 [  0   0  25  34  24   0  27   0   0   0]
 [ 89   8  30   2   1   0   7  11   9   0]
 [ 14  10   0  28   5  20   2   0   0   3]
 [ 34  13  25   0   0  28   5  15   0   0]
 [ 26   0  16  20  18  77   2   7   0   0]]
Total size of testing set
9850
[3558 2294 1480  954  615  393  248  152   99   57]
40
Step 0, Loss 1.2680965662002563
Step 100, Loss 0.20327985286712646
Step 200, Loss -0.48469603061676025
Step 300, Loss -0.9279099702835083
Step 400, Loss -1.2119990587234497
Step 500, Loss -1.3923346996307373
Step 600, Loss -1.506393551826477
Step 700, Loss -1.5777181386947632
Step 800, Loss -1.6216548681259155
Step 900, Loss -1.6477317810058594
Step 1000, Loss -1.6623811721801758
Step 1100, Loss -1.6703828573226929
Step 1200, Loss -1.674989104270935
Step 1300, Loss -1.6772360801696777
Step 1400, Loss -1.6788973808288574
Step 1500, Loss -1.6796221733093262
Step 1600, Loss -1.6796330213546753
Step 1700, Loss -1.6797678470611572
Step 1800, Loss -1.6799665689468384
Step 1900, Loss -1.6802905797958374
Step 2000, Loss -1.679954171180725
Step 2100, Loss -1.679987907409668
Step 2200, Loss -1.6802583932876587
Step 2300, Loss -1.68002188205719
Step 2400, Loss -1.6802165508270264
Step 2500, Loss -1.6802353858947754
Step 2600, Loss -1.6798272132873535
Step 2700, Loss -1.6805100440979004
Step 2800, Loss -1.6802253723144531
Step 2900, Loss -1.6802241802215576
Step 3000, Loss -1.6807197332382202
Step 3100, Loss -1.6803146600723267
Step 3200, Loss -1.6803699731826782
Step 3300, Loss -1.680643081665039
Step 3400, Loss -1.6804919242858887
Step 3500, Loss -1.6805764436721802
Step 3600, Loss -1.6801413297653198
Step 3700, Loss -1.6800439357757568
Step 3800, Loss -1.6805415153503418
Step 3900, Loss -1.680238127708435
Step 4000, Loss -1.6807820796966553
Step 4100, Loss -1.680486798286438
Step 4200, Loss -1.680416464805603
Step 4300, Loss -1.6806986331939697
Step 4400, Loss -1.680809497833252
Step 4500, Loss -1.6800415515899658
Step 4600, Loss -1.6803276538848877
Step 4700, Loss -1.6803888082504272
Step 4800, Loss -1.6805702447891235
Step 4900, Loss -1.680521845817566
Step 5000, Loss -1.680012583732605
Step 5100, Loss -1.6804322004318237
Step 5200, Loss -1.680626392364502
Step 5300, Loss -1.680216908454895
Step 5400, Loss -1.6805306673049927
Step 5500, Loss -1.6801775693893433
Step 5600, Loss -1.6806586980819702
Step 5700, Loss -1.6803300380706787
Step 5800, Loss -1.6806386709213257
Step 5900, Loss -1.6804990768432617
Step 6000, Loss -1.6803088188171387
Step 6100, Loss -1.680645227432251
Step 6200, Loss -1.6808297634124756
Step 6300, Loss -1.680516242980957
Step 6400, Loss -1.6805469989776611
Step 6500, Loss -1.6807501316070557
Step 6600, Loss -1.6806434392929077
Step 6700, Loss -1.680463433265686
Step 6800, Loss -1.6805630922317505
Step 6900, Loss -1.680692195892334
Step 7000, Loss -1.6802347898483276
Step 7100, Loss -1.680760383605957
Step 7200, Loss -1.6806552410125732
Step 7300, Loss -1.680570363998413
Step 7400, Loss -1.6808152198791504
Step 7500, Loss -1.6808903217315674
Step 7600, Loss -1.6803300380706787
Step 7700, Loss -1.6805884838104248
Step 7800, Loss -1.6800005435943604
Step 7900, Loss -1.6807153224945068
Step 8000, Loss -1.680332064628601
Step 8100, Loss -1.68031907081604
Step 8200, Loss -1.680191993713379
Step 8300, Loss -1.6805119514465332
Step 8400, Loss -1.680967092514038
Step 8500, Loss -1.6806597709655762
Step 8600, Loss -1.6807737350463867
Step 8700, Loss -1.6807910203933716
Step 8800, Loss -1.6805726289749146
Step 8900, Loss -1.6806694269180298
Step 9000, Loss -1.6805182695388794
Step 9100, Loss -1.6804448366165161
Step 9200, Loss -1.680368185043335
Step 9300, Loss -1.68087899684906
Step 9400, Loss -1.6807441711425781
Step 9500, Loss -1.6807143688201904
Step 9600, Loss -1.6806368827819824
Step 9700, Loss -1.6809903383255005
Step 9800, Loss -1.680932879447937
Step 9900, Loss -1.680616855621338
Angle Mean: 96.37895202636719, Angle Variance: 0.005015892442315817
Norm Mean: 0.10055243968963623, Norm Variance: 2.5665518599282677e-09
  0%|          | 0/500 [00:00<?, ?it/s]